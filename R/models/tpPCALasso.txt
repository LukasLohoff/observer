model{
  ## hierarchically pooled data model variance
  for(j in 1:t){
    tau_inv[j] ~ dlnorm(mu_tau, tau2_tau)              # pooling prior for data model sqrt(precision)
    tau2[j] <- pow(tau_inv[j], -2)                     # data model precision
  }
  mu_tau ~ dnorm(0, 1 / 1)                           # pooled mean for data model sqrt(precision)
  tau_tau ~ dunif(0, 1)                              # pooled sqrt(precision) for data model sqrt(precision)
  # mu_tau ~ dnorm(0, 1 / 100)                           # pooled mean for data model sqrt(precision)
  # tau_tau ~ dunif(0, 100)                              # pooled sqrt(precision) for data model sqrt(precision)
  tau2_tau <- pow(tau_tau, 2)                          # pooled precision for data model sqrt(precision)
  ## latent variance model 
  s_inv ~ dunif(0, 10)                                # prior for latent principal component sqrt(precision)
  # s_inv ~ dunif(0, 100)                                # prior for latent principal component sqrt(precision)
  s2 <- pow(s_inv, -2)                                 # prior for latent principal component precision
  M_inv <- inverse(tKK / s2 + I_p)                     # scaling matrix for PCA

  for (i in 1:N_total){
    mu_y[i] <- X[H[i], ] %*% K_hat %*% M_inv %*% beta[, tt[i]] / s2
                                                      # data model mean for sparse data format
                                                      # X <- original data matrix
                                                      # K_hat <- PCA rotation matrix
                                                      # H <- observation indicator
    ## hierarchically pooled degress of freedom
    v_inv[i] ~ dchisq(nu[tt[i]])                      # scale mixture for t data model variance 
    v[i] <- (nu[tt[i]] * tau2[tt[i]]) / v_inv[i]      # transformation to scaled inv-Chi squared variance
    tau_y[i] <- pow(v[i] + t(beta[, tt[i]]) %*% M_inv %*% beta[, tt[i]], -1)
                                                      # data model precision for sparse data format
  }

  # lasso priors
  for(j in 1:t){  
    lambda2[j] ~ dgamma(alpha_lambda2, beta_lambda2)  # hierarchically pooled lasso shrinkage parameter
    for(i in 1:p){
      gamma2[i, j] ~ dexp(lambda2[j] / 2)             # lasso scale mixture parameter
      beta[i, j] ~ dnorm(0, 1 / (tau2[j] * nu[j] / (nu[j] - 2) * gamma2[i, j]))  # lasso prior model for tpPCA 
    }
  }

  mu_lambda2 ~ dlnorm(0, 1 / 1)                       # hierarchical mean of lambda2 gamma distribution
  s_lambda2 ~ dunif(0, 1)                            # hierarchical standard deviation of lambda2 gamma distribution
  # mu_lambda2 ~ dlnorm(0, 1 / 100)                       # hierarchical mean of lambda2 gamma distribution
  # s_lambda2 ~ dunif(0, 100)                            # hierarchical standard deviation of lambda2 gamma distribution
  alpha_lambda2 <- pow(mu_lambda2, 2) / pow(s_lambda2, 2) # reparameterization of gamma distribution
  beta_lambda2 <- mu_lambda2 / pow(s_lambda2, 2)       # reparameterization of gamma distribution

  # t degrees of freedom
  for(i in 1:t){
    nu_inv[i] ~ dbeta(alpha_nu, beta_nu)               # pooled inverse degrees of freedom for t data model
    nu[i] <- 2 / nu_inv[i]                             # pooled degrees of freedom for t data model
  }
  mu_nu ~ dbeta(5, 5)                                  # prior for alternative parameterization
  eta_nu ~ dgamma(10, 0.1)                             # prior for alternative parameterization
  alpha_nu <- mu_nu * eta_nu                           # transformation of prior
  beta_nu <- (1 - mu_nu) * eta_nu                      # transformation of prior

  # likelihood
  for (i in 1:N_total){
    Y[i] ~ dnorm(mu_y[i], tau_y[i])                    # t mixture data model using precision
    log_like[i] <- dnorm(Y[i], mu_y[i], tau_y[i])      # t mixture data likelihood using precision 
  }
}